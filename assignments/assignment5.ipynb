{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6a259d2-3cf5-4147-95e1-c7515b71f90d",
   "metadata": {},
   "source": [
    "\n",
    "# que1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f13842b-4aed-4344-820f-841674fb8ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import itertools\n",
    "\n",
    "np.random.seed(42)\n",
    "n_samples = 500\n",
    "X1 = np.random.rand(n_samples, 1) * 10\n",
    "X2 = X1 + np.random.normal(0, 0.5, (n_samples, 1))\n",
    "X3 = X1 * 0.5 + np.random.normal(0, 0.3, (n_samples, 1))\n",
    "X4 = X1 * 2 + np.random.normal(0, 1, (n_samples, 1))\n",
    "X5 = X1 + X2 + np.random.normal(0, 0.5, (n_samples, 1))\n",
    "X6 = X3 + X4 + np.random.normal(0, 0.5, (n_samples, 1))\n",
    "X7 = X1 * 3 + np.random.normal(0, 1, (n_samples, 1))\n",
    "X = np.hstack((X1, X2, X3, X4, X5, X6, X7))\n",
    "y = 3*X1 + 2*X2 - X3 + 0.5*X4 + np.random.normal(0, 2, (n_samples, 1))\n",
    "y = y.ravel()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "502c3595-4df3-4a1d-8487-33ecffe003fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression_gd(X, y, lr=0.01, lambda_=0.1, epochs=1000):\n",
    "    m, n = X.shape\n",
    "    X_b = np.c_[np.ones((m, 1)), X]\n",
    "    theta = np.zeros((n + 1, 1))\n",
    "    y = y.reshape(-1, 1)\n",
    "    for _ in range(epochs):\n",
    "        y_pred = X_b.dot(theta)\n",
    "        error = y_pred - y\n",
    "        gradient = (1/m) * (X_b.T.dot(error) + lambda_ * np.r_[[[0]], theta[1:]])\n",
    "        theta -= lr * gradient\n",
    "        if np.isnan(theta).any() or np.isinf(theta).any():\n",
    "            return None\n",
    "    return theta\n",
    "\n",
    "def ridge_cost(X, y, theta, lambda_):\n",
    "    m = len(y)\n",
    "    X_b = np.c_[np.ones((m, 1)), X]\n",
    "    y_pred = X_b.dot(theta)\n",
    "    return (1/(2*m)) * np.sum((y_pred - y.reshape(-1,1))**2) + (lambda_/(2*m)) * np.sum(theta[1:]**2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c91d8f7e-4cda-489c-a132-6eb7a1801e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Learning Rate        Lambda       Cost  R2_Score\n",
      "17          0.010  0.000000e+00   2.232516  0.982769\n",
      "14          0.010  1.000000e-10   2.232516  0.982769\n",
      "15          0.010  1.000000e-05   2.232517  0.982769\n",
      "16          0.010  1.000000e-03   2.232580  0.982768\n",
      "18          0.010  1.000000e+00   2.296009  0.982718\n",
      "19          0.010  1.000000e+01   2.835258  0.982284\n",
      "20          0.010  2.000000e+01   3.388439  0.981853\n",
      "13          0.001  2.000000e+01  10.470367  0.912590\n",
      "12          0.001  1.000000e+01   9.981860  0.912503\n",
      "11          0.001  1.000000e+00   9.539018  0.912403\n",
      "\n",
      "Best Parameters:\n",
      " Learning Rate    0.010000\n",
      "Lambda           0.000000\n",
      "Cost             2.232516\n",
      "R2_Score         0.982769\n",
      "Name: 17, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "learning_rates = [0.0001, 0.001, 0.01]\n",
    "lambdas = [1e-10, 1e-5, 1e-3, 0, 1, 10, 20]\n",
    "results = []\n",
    "\n",
    "for lr, lam in itertools.product(learning_rates, lambdas):\n",
    "    theta = ridge_regression_gd(X_train, y_train, lr=lr, lambda_=lam, epochs=2000)\n",
    "    if theta is None:\n",
    "        continue\n",
    "    cost = ridge_cost(X_train, y_train, theta, lam)\n",
    "    X_test_b = np.c_[np.ones((X_test.shape[0], 1)), X_test]\n",
    "    y_pred = X_test_b.dot(theta)\n",
    "    if np.isnan(y_pred).any() or np.isinf(y_pred).any():\n",
    "        continue\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    results.append((lr, lam, cost, r2))\n",
    "\n",
    "if results:\n",
    "    df_results = pd.DataFrame(results, columns=[\"Learning Rate\", \"Lambda\", \"Cost\", \"R2_Score\"])\n",
    "    best = df_results.loc[df_results[\"R2_Score\"].idxmax()]\n",
    "    print(df_results.sort_values(by=\"R2_Score\", ascending=False).head(10))\n",
    "    print(\"\\nBest Parameters:\\n\", best)\n",
    "else:\n",
    "    print(\"No valid parameter combinations found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51531e3b-14df-42c1-878a-c8eeef31b2b1",
   "metadata": {},
   "source": [
    "# que2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff27b540-f5d0-4c74-8b76-1e269453338b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Model  R2 Score            MSE\n",
      "0  Linear Regression  0.290745  128284.345497\n",
      "1   Ridge Regression  0.299789  126648.594229\n",
      "2   Lasso Regression  0.299065  126779.466413\n",
      "\n",
      "Best Performing Model: Ridge Regression\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Win10\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.247e+04, tolerance: 4.367e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "data = pd.read_csv(\"C:\\\\Users\\\\Win10\\\\Downloads\\\\Hitters (1).csv\")\n",
    "\n",
    "data = data.dropna()\n",
    "\n",
    "categorical_cols = data.select_dtypes(include=['object']).columns\n",
    "data = pd.get_dummies(data, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "X = data.drop(\"Salary\", axis=1)\n",
    "y = data[\"Salary\"]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "linear = LinearRegression()\n",
    "linear.fit(X_train, y_train)\n",
    "y_pred_linear = linear.predict(X_test)\n",
    "r2_linear = r2_score(y_test, y_pred_linear)\n",
    "mse_linear = mean_squared_error(y_test, y_pred_linear)\n",
    "\n",
    "ridge = Ridge(alpha=0.5748)\n",
    "ridge.fit(X_train, y_train)\n",
    "y_pred_ridge = ridge.predict(X_test)\n",
    "r2_ridge = r2_score(y_test, y_pred_ridge)\n",
    "mse_ridge = mean_squared_error(y_test, y_pred_ridge)\n",
    "\n",
    "lasso = Lasso(alpha=0.5748)\n",
    "lasso.fit(X_train, y_train)\n",
    "y_pred_lasso = lasso.predict(X_test)\n",
    "r2_lasso = r2_score(y_test, y_pred_lasso)\n",
    "mse_lasso = mean_squared_error(y_test, y_pred_lasso)\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    \"Model\": [\"Linear Regression\", \"Ridge Regression\", \"Lasso Regression\"],\n",
    "    \"R2 Score\": [r2_linear, r2_ridge, r2_lasso],\n",
    "    \"MSE\": [mse_linear, mse_ridge, mse_lasso]\n",
    "})\n",
    "\n",
    "print(results)\n",
    "best_model = results.loc[results[\"R2 Score\"].idxmax(), \"Model\"]\n",
    "print(\"\\nBest Performing Model:\", best_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972bb9e2-0a70-40ff-b358-a1d2863e94e5",
   "metadata": {},
   "source": [
    "# que3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c54f6bd6-4aae-4df7-a880-2ea95b5d9653",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Win10\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 37.62056365606986, tolerance: 2.1278717316506466\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge Best Alpha: 20.0\n",
      "Ridge R2 Score: 0.6062156400080905\n",
      "Lasso Best Alpha: 0.001\n",
      "Lasso R2 Score: 0.6061906372004509\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.linear_model import RidgeCV, LassoCV\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "data = fetch_california_housing()\n",
    "X, y = data.data, data.target\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "ridge = RidgeCV(alphas=[1e-15, 1e-10, 1e-5, 1e-3, 0.1, 1, 10, 20], cv=5)\n",
    "ridge.fit(X_scaled, y)\n",
    "ridge_pred = ridge.predict(X_scaled)\n",
    "ridge_r2 = r2_score(y, ridge_pred)\n",
    "\n",
    "lasso = LassoCV(alphas=[1e-15, 1e-10, 1e-5, 1e-3, 0.1, 1, 10, 20], cv=5, max_iter=10000)\n",
    "lasso.fit(X_scaled, y)\n",
    "lasso_pred = lasso.predict(X_scaled)\n",
    "lasso_r2 = r2_score(y, lasso_pred)\n",
    "\n",
    "print(\"Ridge Best Alpha:\", ridge.alpha_)\n",
    "print(\"Ridge R2 Score:\", ridge_r2)\n",
    "print(\"Lasso Best Alpha:\", lasso.alpha_)\n",
    "print(\"Lasso R2 Score:\", lasso_r2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a17b0c1-0be1-411f-9395-eaf7807cc79b",
   "metadata": {},
   "source": [
    "# que4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fde593fd-0d32-42c4-a614-6921b36f52f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiclass Logistic Regression Accuracy: 0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "model = OneVsRestClassifier(LogisticRegression(max_iter=1000))\n",
    "model.fit(X_train_scaled, y_train)\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(\"Multiclass Logistic Regression Accuracy:\", acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb764576-3d02-4b63-aa90-55f40dc8d1c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
